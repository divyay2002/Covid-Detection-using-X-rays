{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebad23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_PATH = \"C:/Users/TAREESH/Desktop/covid/CovidDataset-20200427T133042Z-001/CovidDataset/Train\"\n",
    "VAL_PATH = \"C:/Users/TAREESH/Desktop/covid/CovidDataset-20200427T133042Z-001/CovidDataset/Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47901498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66723164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building CNN based model in keras\n",
    "#we will be building 3/4 CNN layers followed by classification layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', input_shape = (224,224,3))) #32 number of filters -> we use 3X3 as it is standard\n",
    "#So 1stly we are puting our convolutional layer with 32 number of filters we are keeping the no of filters \n",
    "#as small in the begining because the lower layers detect features in very small part of the image \n",
    "#32 filters = 32 feature Extractors\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "#adding drop out to find over fitting\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#in output layer we will be having only one neuron which performs binary classification\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = keras.losses.binary_crossentropy, optimizer = 'adam', metrics = ['accuracy'])\n",
    "#adam - is a default choice used for optimising ; we do gradient decent optimiser for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7313f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 220, 220, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 110, 110, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 110, 110, 64)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 108, 108, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 54, 54, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 26, 26, 128)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                5537856   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,668,097\n",
      "Trainable params: 5,668,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1154d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train from scratch\n",
    "#we will use the keras image data generator library to make the data ready for the model\n",
    "\n",
    "\n",
    "\n",
    "train_datagen = image.ImageDataGenerator( # we will do some augmentation on the image as well\n",
    "    rescale = 1./255, # rescale the data by the factor of 1/255 which will help us to do the normalization\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2, #shear and zoom will allow us to take some random crops from the drainages and \n",
    "                      #those will be like you are zooming into some part of image and the magnitude is around 20%\n",
    "    horizontal_flip = True\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = image.ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e9c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 224 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator =  train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'             \n",
    ")\n",
    "#224 image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e08843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Covid': 0, 'Normal': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb68cbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_dataset.flow_from_directory(\n",
    "    VAL_PATH,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2980b40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TAREESH\\AppData\\Local\\Temp/ipykernel_3616/2593354632.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hist = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7/8 [=========================>....] - ETA: 27s - loss: 2.2977 - accuracy: 0.5223WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1600 batches). You may need to use the repeat() function when building your dataset.\n",
      "8/8 [==============================] - 221s 26s/step - loss: 2.2977 - accuracy: 0.5223 - val_loss: 0.6793 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# now we here going to train the model\n",
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 8,\n",
    "    epochs = 200,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1701d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TAREESH\\AppData\\Local\\Temp/ipykernel_3616/3153451632.py:2: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model.evaluate_generator(train_generator)\n",
      "C:\\Users\\TAREESH\\AppData\\Local\\Temp/ipykernel_3616/3153451632.py:3: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model.evaluate_generator(validation_generator)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6792684197425842, 0.5]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"model_adv.h5\")\n",
    "model.evaluate_generator(train_generator)\n",
    "model.evaluate_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "227ad0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_adv.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c533e5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Covid': 0, 'Normal': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24cdaaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27bccccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[[1]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[1]]\n",
      "[[1]]\n",
      "[[1]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[1]]\n",
      "[[0]]\n",
      "[[1]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[1]]\n",
      "[[1]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(\"C:/Users/TAREESH/Desktop/covid/CovidDataset-20200427T133042Z-001/CovidDataset/Val/Normal\"):\n",
    "    img = image.load_img(\"C:/Users/TAREESH/Desktop/covid/CovidDataset-20200427T133042Z-001/CovidDataset/Val/Normal/\"+i, target_size = (224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "#     print(img)\n",
    "    p = (model.predict(img) > 0.5).astype(\"int32\")\n",
    "    print(p)\n",
    "    y_test = np.append(y_test,int(p[0]))\n",
    "    y_actual= np.append(y_actual,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "926e7216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(\"C:/Users/TAREESH/Desktop/covid/CovidDataset-20200427T133042Z-001/CovidDataset/Val/Covid\"):\n",
    "    img = image.load_img(\"C:/Users/TAREESH/Desktop/covid/CovidDataset-20200427T133042Z-001/CovidDataset/Val/Covid/\"+i, target_size = (224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    p = (model.predict(img) > 0.5).astype(\"int32\")\n",
    "    print(p)\n",
    "    y_test = np.append(y_test,p[0])\n",
    "    y_actual = np.append(y_actual,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae45ab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_actual = np.array(y_actual)\n",
    "y_test  = np.array(y_test)\n",
    "len(y_actual), len(y_test)\n",
    "print(y_actual, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9e01a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(len(y_actual), len(y_test))\n",
    "cm = confusion_matrix(y_actual, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf62fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0]\n",
      " [21  9]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02ad6fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm, cmap = 'plasma', annot = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
